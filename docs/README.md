# Hadoop vs Spark: Comparison Report

This document summarizes the end-to-end comparison between Apache Hadoop (MapReduce) and Apache Spark for computing indegree and indegree distribution on large graphs in this project.

It covers correctness of results, execution performance, and system design considerations, and shows how to reproduce the checks with the scripts included here.

## Datasets and jobs compared

- Datasets: email-EuAll, web-BerkStan, soc-LiveJournal1
- Jobs: indegree and distribution
- Outputs compared between:
  - Hadoop: `data/results/hadoop/<dataset>/{indegree,distribution}/part-r-00000`
  - Spark: `data/results/spark/<dataset>/{indegree,distribution}/part-*`

## How to reproduce the comparison

Run these inside the `spark-master` container.

- Validate correctness (Spark vs Hadoop outputs):

```bash
python3 /data/scripts/validate_results.py
```

- Summarize performance from `data/metrics/summary.csv`:

```bash
python3 /data/scripts/perf_compare.py
```

- Metrics plots are auto-generated by:

```bash
python3 /data/scripts/plot_metrics.py
```

Results are stored under `data/results/plots/metrics_*.png`.

## Correctness of results

All datasets and both jobs matched exactly between Hadoop and Spark after normalization:

- email-EuAll: distribution = match, indegree = match
- web-BerkStan: distribution = match, indegree = match
- soc-LiveJournal1: distribution = match, indegree = match

The validator (`scripts/validate_results.py`) concatenates Spark part files and compares them against Hadoop output using multiset (Counter) equality.

## Execution performance highlights

Numbers are derived from `data/metrics/summary.csv` (Hadoop totals sum two phases; averages are phase-means). Memory notes below.

- email-EuAll
  - Elapsed: Spark 11.90s vs Hadoop 8.52s → Hadoop faster (−3.38s)
  - Avg CPU util: Spark 30.22% vs Hadoop ~15.44%
  - Max RSS: Spark 391,308 KB; Hadoop sum 758,208 KB
  - Disk r/w (kB/s): Spark 106.86/0.00; Hadoop ~52.95/1.21
  - Net rx/tx (kB/s): Spark 0.09/0.01; Hadoop ~13.57/9.90

- web-BerkStan
  - Elapsed: Spark 19.24s vs Hadoop 28.97s → Spark faster (+9.73s)
  - Avg CPU util: Spark 36.47% vs Hadoop ~17.49%
  - Max RSS: Spark 464,260 KB; Hadoop sum 1,858,236 KB
  - Disk r/w (kB/s): Spark 52.00/0.00; Hadoop ~1.87/1.78
  - Net rx/tx (kB/s): Spark 0.11/0.01; Hadoop ~12.69/9.26

- soc-LiveJournal1
  - Elapsed: Spark 217.72s vs Hadoop 379.22s → Spark faster (+161.50s)
  - Avg CPU util: Spark 88.62% vs Hadoop ~12.90%
  - Max RSS: Spark 761,504 KB; Hadoop sum 2,776,680 KB
  - Disk r/w (kB/s): Spark 1,997.73/0.00; Hadoop ~0.51/6.24
  - Net rx/tx (kB/s): Spark 0.09/0.01; Hadoop ~1.14/0.86

Notes and caveats:

- Hadoop memory “sum” above is conservative; a phase-level maximum would be a fairer comparator than summing max RSS across phases.
- Spark network averages near zero likely reflect driver-local execution in this Docker setup; distributed multi-node runs will show higher values.
- Where `dstat` produced header-only CSVs on Spark, we enabled a robust `sar` fallback; `aggregate_metrics.py` prefers `dstat` samples, otherwise uses `sar` averages.

Plots:

- See `data/results/plots/metrics_*.png` for bar charts of elapsed time, max RSS, and avg CPU/disk/net per dataset and framework.

## System design and data processing approach

- Execution model
  - Hadoop (MapReduce on YARN): Disk-oriented, each MR stage materializes to HDFS; robust and durable; higher I/O overhead between stages.
  - Spark (Standalone here): In-memory DAG engine (RDD/DataFrame). Lazy transformations, pipeline fusion, fewer materializations; well-suited for iterative and multi-stage analytics.

- Data abstractions
  - Hadoop: Map/Reduce over key-value pairs; each stage is a separate job with an HDFS boundary.
  - Spark: RDD/DataFrame with lineage; narrow vs wide dependencies optimize pipelining and shuffle; caching enables reuse.

- Fault tolerance
  - Hadoop: Durable HDFS checkpoints; retries per task; strong batch guarantees.
  - Spark: Lineage-based recomputation and optional checkpointing; typically faster recovery for iterative workloads.

- Shuffle and I/O
  - Hadoop: Sort-based shuffle and HDFS writes occur at each job boundary.
  - Spark: Shuffle on wide dependencies only; can spill to disk but benefits from memory and optimized shuffle services.

- Resource management
  - Hadoop MR on YARN: mature multi-tenant scheduling and isolation.
  - Spark: Supports Standalone (here), YARN, or Kubernetes; flexible for mixed batch/interactive/streaming.

- When to prefer which
  - Hadoop MapReduce: Massive, simple batch ETL where durability and simplicity outweigh latency; organizations standardized on YARN+HDFS.
  - Spark: Iterative analytics, graph processing, ML, and pipelines sensitive to end-to-end latency and multi-stage optimization.

## Artifacts and scripts

- Correctness validator: `scripts/validate_results.py`
- Performance summary: `scripts/perf_compare.py`
- Metrics aggregator and plots: `scripts/aggregate_metrics.py`, `scripts/plot_metrics.py`
- Wrapper and samplers: `scripts/with_metrics.sh` (GNU time + dstat + sar fallback)

## Next steps (optional)

- Refine memory comparison by using a phase-level maximum for Hadoop instead of summing phase maxima.
- Add a Markdown export of charts and summary stats for reporting.
- Run on a multi-worker Spark cluster to capture realistic network throughput.
